# -*- coding: utf-8 -*-
"""Neha_Naik_Task2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oOfktfFmAT3PbpqNj9_Sh4GWXfSTESua

The K-Nearest Neighbors (KNN) algorithm is a good choice for this problem due to the following reasons:

Simplicity: KNN is easy to understand and implement, making it a suitable choice for initial exploration of the dataset.

Non-parametric: KNN doesn't assume any specific distribution of the data, which is useful as we might not know the underlying distribution of the 'target' variable.

Versatility: KNN can be used for both classification (predicting discrete labels) and regression (predicting continuous values), offering flexibility in addressing different problem formulations.

Adaptability: KNN can adapt to changes in the data as it simply relies on the nearest neighbors for prediction.
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from sklearn import preprocessing
# %matplotlib inline

df=pd.read_csv('train.csv')

df.head()

def convert_non_numerical(input_string):
  result = ''
  for char in input_string:
    if char.isnumeric():
      result += char
    else:
      result += str(ord(char))
  return result

df['target'] = df['target'].astype(str)

df['target'] = df['target'].apply(convert_non_numerical)

df['target']=df['target'].astype(int)

df.head()

target_counts = df['target'].value_counts()

for value, count in target_counts.items():
    print(f"Value: {value}, Count: {count}")

unique_count = df['target'].nunique()
print("Number of unique target values:", unique_count)

df.columns

df.info()

# Select all columns except 'target'
df_without_target = df.drop('target', axis=1)
X = df_without_target.to_numpy()

y = df['target'].values
y[0:5]

X = preprocessing.StandardScaler().fit(X).transform(X.astype(float))
X[0:5]

from sklearn.neighbors import KNeighborsClassifier

k = 4
#Train Model and Predict
neigh = KNeighborsClassifier(n_neighbors = k).fit(X,y)

test_data=pd.read_csv('test.csv')

X_test = test_data.drop('target', axis=1, errors='ignore').to_numpy()

# Preprocess the test data using the same scaler used for training data
X_test = preprocessing.StandardScaler().fit(X_test).transform(X_test.astype(float))

# Use the trained KNeighborsClassifier model to make predictions
y_pred = neigh.predict(X_test)

# Print or store the predicted target values
print(y_pred)

def convert_digits_to_letters(number):
  number_str = str(number)
  first_digit_char = chr(int(number_str[0:2]))
  result = first_digit_char + number_str[2:]
  return result

# Example usage:
converted_number = convert_digits_to_letters(6674)
print(converted_number)  # Output: B74

# Add a new column 'predicted_target' to the test_data DataFrame
test_data['predicted_target'] = y_pred

# Display the updated DataFrame
test_data.head()

test_data['predicted_target'] = test_data['predicted_target'].apply(convert_digits_to_letters)

# Display the updated DataFrame
test_data.head()

#To test the model accuracy , divide the train data into 2 components
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)
print ('Train set:', x_train.shape,  y_train.shape)
print ('Test set:', x_test.shape,  y_test.shape)

yhat = neigh.predict(x_test)
yhat[0:5]

yhat = neigh.predict(x_test)

# Create a DataFrame for comparison
comparison_df = pd.DataFrame({'Actual': y_test, 'Predicted': yhat})

# Convert numerical predictions back to original format if needed
comparison_df['Actual'] = comparison_df['Actual'].apply(convert_digits_to_letters)
comparison_df['Predicted'] = comparison_df['Predicted'].apply(convert_digits_to_letters)

comparison_df.head()

from sklearn import metrics
print("Train set Accuracy Percentage : ", 100*metrics.accuracy_score(y_train, neigh.predict(x_train)))
print("Test set Accuracy Percentage : ", 100*metrics.accuracy_score(y_test, yhat))